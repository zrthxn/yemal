{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Crypto.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWh_uH3QYY8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "a946ab0e-458a-44c4-ffe1-da1bcb27b68e"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9e6Isz_XLDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from collections import deque\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "\n",
        "DATADIR = \"\"\n",
        "\n",
        "SEQUENCE_LEN = 50  # Number of previous data points to use in RNN\n",
        "FUTURE_LEN = 5  # Number of future data points to predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvPU72BIXLEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "dcdd5083-9b9e-46fa-fe37-fa4773510527"
      },
      "source": [
        "# Reading the data\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "dataset = pd.read_csv( os.path.join(DATADIR, \"ETH-USD.csv\"), names=['time', 'low', 'high', 'open', 'close', 'volume'] )\n",
        "dataset.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
        "dataset.dropna(inplace=True)\n",
        "dataset.set_index(\"time\", inplace=True)\n",
        "\n",
        "dataset['future'] = dataset['close'].shift(-FUTURE_LEN)\n",
        "\n",
        "\n",
        "def increaseOrDecrease(past, future):\n",
        "    # Figure out whether the value increses or decreases\n",
        "    if float(past) < float(future):\n",
        "        return 1  # increase\n",
        "    else:\n",
        "        return 0  # decrease or static\n",
        "\n",
        "\n",
        "dataset['trend'] = list(map(increaseOrDecrease, dataset['close'], dataset['future']))\n",
        "\n",
        "val_split = 0.05\n",
        "val_split_indices = sorted(dataset.index.values)[-int( val_split * len(sorted(dataset.index.values)) )]\n",
        "validation_dataset = dataset[(dataset.index >= val_split_indices )]\n",
        "\n",
        "discard_indices = sorted(dataset.index.values)[-int( 5 )]  # Discard NaN values due to shift\n",
        "validation_dataset = validation_dataset[(validation_dataset.index <= discard_indices)]\n",
        "\n",
        "dataset = dataset[(dataset.index < val_split_indices)]\n",
        "\n",
        "print(dataset.head())\n",
        "\n",
        "# Drop unnecessary columns\n",
        "validation_dataset = validation_dataset.drop(\"low\", 1)\n",
        "validation_dataset = validation_dataset.drop(\"high\", 1)\n",
        "validation_dataset = validation_dataset.drop(\"open\", 1)\n",
        "validation_dataset = validation_dataset.drop(\"future\", 1)\n",
        "\n",
        "dataset = dataset.drop(\"low\", 1)\n",
        "dataset = dataset.drop(\"high\", 1)\n",
        "dataset = dataset.drop(\"open\", 1)\n",
        "dataset = dataset.drop(\"future\", 1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  low   high        open  ...     volume     future  trend\n",
            "time                                      ...                             \n",
            "1528968720  485.98999  486.5  486.019989  ...  26.019083  485.98999      0\n",
            "1528968780  486.00000  486.0  486.000000  ...   8.449400  485.98999      0\n",
            "1528968840  485.75000  486.0  486.000000  ...  26.994646  485.98999      1\n",
            "1528968900  485.75000  486.0  485.750000  ...  77.355759  485.98999      0\n",
            "1528968960  485.98999  486.0  486.000000  ...   7.503300  486.00000      0\n",
            "\n",
            "[5 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC-S--7QXLEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Data Preprocessing\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def preprocess(df):\n",
        "    for col in df.columns:\n",
        "        if col != \"trend\":\n",
        "            df[col] = df[col].pct_change()\n",
        "            df.dropna(inplace=True)\n",
        "            df[col] = preprocessing.scale(df[col].values)\n",
        "        \n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Contains all sequences of required size\n",
        "    sequences = []\n",
        "    # Current sequence, deque pushes out old data as new entries are appended\n",
        "    seq = deque(maxlen=SEQUENCE_LEN)\n",
        "\n",
        "    for i in df.values:\n",
        "        seq.append([n for n in i[:-1]])  # Append everything except last column\n",
        "        if len(seq) == SEQUENCE_LEN:\n",
        "            sequences.append([np.array(seq), i[-1]])\n",
        "            # Add the current sequence\n",
        "    \n",
        "    # Shuffle order of sequences\n",
        "    random.shuffle(sequences)\n",
        "\n",
        "    # Splitting into buys and sells\n",
        "    buys = []\n",
        "    sells = []\n",
        "\n",
        "    for _seq, trend in sequences:\n",
        "        if trend == 0:\n",
        "            buys.append([_seq, trend])\n",
        "        elif trend == 1:\n",
        "            sells.append([_seq, trend])\n",
        "\n",
        "    random.shuffle(buys)\n",
        "    random.shuffle(sells)\n",
        "\n",
        "    # Equalize the two\n",
        "    lower = min(len(buys), len(sells))\n",
        "    buys = buys[:lower]\n",
        "    sells = sells[:lower]\n",
        "\n",
        "    # Rejoin into main\n",
        "    sequences = buys + sells\n",
        "    random.shuffle(sequences)\n",
        "\n",
        "    # FINALLY Split into features and labels\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for _seq, trend in sequences:\n",
        "        X.append(_seq)\n",
        "        y.append(trend)\n",
        "\n",
        "    X = np.array(X)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "train_X, train_y = preprocess(dataset)\n",
        "validation_X, validation_y = preprocess(validation_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d4RUmjzXLEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYnTN9WJXLEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "2ab2e4bd-223d-4774-b807-d5b24562605f"
      },
      "source": [
        "# Creating the Model\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Sequential Model\n",
        "model = Sequential()\n",
        "\n",
        "# Layer 1 :: Sheets = LSTM, Dropout, Normalize\n",
        "# Input layer from the sequences\n",
        "model.add(CuDNNLSTM(128, input_shape=(train_X.shape[1:]), return_sequences=True ))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Layer 2 :: Sheets = LSTM, Dropout, Normalize\n",
        "model.add(CuDNNLSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Layer 3 :: Sheets = LSTM, Dropout, Normalize\n",
        "# Final layer\n",
        "model.add(CuDNNLSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Layer 4 :: Sheets = Dense, Dropout\n",
        "# Collection layer, from 128 to 32 nodes\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "LEARNING_DECAY = 1e-6\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "opt = Adam(lr=LEARNING_RATE, decay=LEARNING_DECAY)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0830 17:19:37.366574 140273031346048 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmmstEBNXLEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "254d6c1c-9414-4a4f-e38c-9521a3c61e44"
      },
      "source": [
        "# Compile model\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# train_X = train_X + train_X\n",
        "# train_y = train_y + train_y\n",
        "\n",
        "print(len(train_X.(train_X)))\n",
        "\n",
        "# Train Model\n",
        "# model.fit(\n",
        "#     train_X, train_y,\n",
        "#     batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "#     validation_data=(validation_X, validation_y)\n",
        "# )"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9vwC0OUjTZA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "bdfa4d68-5c03-4c69-92ac-2000bb473e98"
      },
      "source": [
        "for n in range(100, 110):\n",
        "    pr = model.predict(train_X)\n",
        "    print(\"Actual\", int(train_y[n]))\n",
        "    print(\"Prediction\", (pr[0]))\n",
        "    print(\"============\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual 1\n",
            "Prediction [0.5038789  0.49612108]\n",
            "============\n",
            "Actual 1\n",
            "Prediction [0.5038789  0.49612108]\n",
            "============\n",
            "Actual 0\n",
            "Prediction [0.5038789  0.49612108]\n",
            "============\n",
            "Actual 1\n",
            "Prediction [0.5038789  0.49612108]\n",
            "============\n",
            "Actual 0\n",
            "Prediction [0.5038789  0.49612108]\n",
            "============\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-02615b052e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"============\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}